{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc660767",
   "metadata": {
    "id": "dc660767"
   },
   "source": [
    "<div id=\"container\" style=\"position:relative;\">\n",
    "<div style=\"float:left\">\n",
    "\n",
    "***Kazi Shahid***\n",
    "\n",
    "***BrainStation Data Science Diploma Candidate***\n",
    "\n",
    "***Capstone Project***\n",
    "\n",
    "=============================================================\n",
    "\n",
    "***Project SteamBuzz: Will Our Game Create a Buzz in the Steam community?***\n",
    "\n",
    "***Part 4 (d): Sentiment Analysis ML Model 4 - Support Vector Machines***\n",
    "</div>\n",
    "<div style=\"position:relative; float:right\"><img style=\"height:100px\" src =\"https://i.ibb.co/mcvpL4Z/Steam-Buzz-logo.png\" />\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73ddd67",
   "metadata": {
    "id": "b73ddd67"
   },
   "source": [
    "---\n",
    "# Overview\n",
    "\n",
    "In this part of the project, we will train a Support Vector classifier (\"SVC\") on the data.\n",
    "\n",
    "A Support Vector Machine, or SVM, is a classifier that finds an optimal hyperplane that maximizes the margin between two classes. Though it can also be applied as a multi-class classifier by running the necessary number of binary \"one-versus-rest\" SVMs, it is still basically a binary classifier run several times in permutation. A SVM strives to find a line that is in the middle of / evenly spaced between the two classes. This is achieved by maximizing the distance between the decision boundary and the closest points. The perpendicular line from the decision boundary to the closest points in both classes is called Support Vector, which a SVM tries to maximize the length of.\n",
    "\n",
    "A SVC is very computationally expensive though. We will still give it a run to see if an efficient processing can be reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8a41cf",
   "metadata": {
    "id": "1c8a41cf"
   },
   "source": [
    "---\n",
    "# Process Flow\n",
    "\n",
    "The intended process flow for this part of the project is as follows:\n",
    "\n",
    "1. Loading the dataset as was prepared and preprocessed in Part 3 of the project\n",
    "2. Employing the classifier and optimizing its hyperparameters through Grid Search and cross validation\n",
    "3. Choosing the parameter values for which the classifier performed the best and re-employing the classifier with the optimized hyperparameters\n",
    "4. Evaluating the model using the appropriate performance measures\n",
    "5. Derive any valuable insights from the model\n",
    "6. Wrapping up with concluding remarks, summarizing the findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9cdb1af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T00:30:43.267727Z",
     "start_time": "2021-10-05T00:30:41.234775Z"
    },
    "id": "d9cdb1af"
   },
   "outputs": [],
   "source": [
    "# Importing the necessary data analysis and visualization toolkits\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# To display ALL the columns in the dataframes\n",
    "pd.options.display.max_columns=None\n",
    "\n",
    "# To display a considerable extent (first 500 characters) of the content of each column of the dataframes\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# Filtering out potential warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e39a0b",
   "metadata": {
    "id": "50e39a0b"
   },
   "source": [
    "---\n",
    "# Loading the Training and Test Datasets\n",
    "\n",
    "SVC is a quite computationally-expensive classifier, and our first run of it (fitted to the full sets) had to be terminated after almost a day of running. Instead, in order to gain some insights in time from this model rather than none, we opt for the PCA-transformed X train and test datasets, which still covers 80% of the variance of data and therefore the insights gained from the model trained on it should not be unreasonably far off from the same derived from the full datasets.\n",
    "\n",
    "Therefore, instead of the full X train and test sets (includes all features), we will load the PCA-transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b160489",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-03T15:31:01.038775Z",
     "start_time": "2021-10-03T15:30:08.964089Z"
    },
    "id": "4b160489"
   },
   "outputs": [],
   "source": [
    "# Importing the X_train, X_test, y_train, and y_test datasets from the respective pickle files into Pandas DataFrame forms\n",
    "X_train = pd.read_pickle(\"data\\\\x_train.pkl\")\n",
    "X_test = pd.read_pickle(\"data\\\\x_test.pkl\")\n",
    "y_train = pd.read_pickle(\"data\\y_train.pkl\")\n",
    "y_test = pd.read_pickle(\"data\\y_test.pkl\")\n",
    "\n",
    "# Note: The destination paths above includes a duplicated backslash (\"\\\\\") rather than single (\"\\\") as otherwise it shows the below error\n",
    "# \"SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 4-5: truncated \\xXX escape\"\n",
    "# This is because the \"\\x...\" (\"\\x_train\" in our case) starts an 8-character Unicode escape where digits follow the \"\\x\"\n",
    "# But in our case, digits do not follow the \"\\x...\" (\"\\x_train\" in our case), making the escape invalid and throwing an error\n",
    "# This error has been resolved based on https://stackoverflow.com/a/1347854"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b01dc933",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-03T15:31:39.328499Z",
     "start_time": "2021-10-03T15:31:39.310520Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b01dc933",
    "outputId": "643ef63c-c8e8-4c7f-8fa1-c5f94068957e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train dataset: (55248, 876)\n",
      "Shape of X_test dataset: (13812, 876)\n",
      "Shape of y_train dataset: (55248,)\n",
      "Shape of y_test dataset: (13812,)\n"
     ]
    }
   ],
   "source": [
    "# Checking that the datasets loaded correctly, displaying the shapes\n",
    "## Displaying the dataframes themselves take a lot of time, hence choosing to display their shapes\n",
    "print(f\"Shape of X_train dataset: {X_train.shape}\")\n",
    "print(f\"Shape of X_test dataset: {X_test.shape}\")\n",
    "print(f\"Shape of y_train dataset: {y_train.shape}\")\n",
    "print(f\"Shape of y_test dataset: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1878aab9",
   "metadata": {
    "id": "1878aab9"
   },
   "source": [
    "The shapes of the four sets match the outputs in Part 3 of the project. We can proceed with working them into our ML model in this part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1432e299",
   "metadata": {
    "id": "1432e299"
   },
   "source": [
    "# Selection of Hyperparameters for Support Vector Classifier\n",
    "\n",
    "The definition and overview of Support Vector Machine (SVM) has been discussed in Part 4 of this project. A full list and description of the hyperparameters we can tune in a SVM can be found in [its documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).\n",
    "\n",
    "We will consider setting / tuning the three most impactful and important hyperparameters for a Support Vector Classifier (SVC), as discussed below.\n",
    "\n",
    "\n",
    "## Kernel Method\n",
    "\n",
    "The Kernel Method (aka Kernel Trick) transforms data that is not linearly separable in an n-dimensional space to a higher dimension where it is linearly separable. An example of it is given below, where in the left picture we cannot separate the red cluster of data in the middle using a plane, but once the data points are transformed as in the second picture, we can use a plane to separate the red cluster from the green cluster of data:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*mCwnu5kXot6buL7jeIafqQ.png\">\n",
    "\n",
    "*(Further discussion on Kernel Trick, along with the image above, can be found in [this article on Medium](https://medium.com/@zxr.nju/what-is-the-kernel-trick-why-is-it-important-98a98db0961d).)*\n",
    "\n",
    "The parameter options for kernel trick include `linear` for linear hyperplane (i.e., for two-dimensional data, a line), and `rbf` and `poly` for non-linear hyperplanes.\n",
    "\n",
    "[The `linear` kernel is generally recommended for text classification](https://www.svm-tutorial.com/2014/10/svm-linear-kernel-good-text-classification/), for the following reasons:\n",
    "\n",
    "- Most text classification problems are linearly separable.\n",
    "- The linear kernel works well in case of a large set of features in the data, which is most often the case in text classification problems as we vectorize the text data into hundreds / thousands of features.\n",
    "- Linear kernel is faster than the other kernels when training a SVM model.\n",
    "- With linear kernel, we need to optimize only the penalty strength parameter, where with other kernels an additional parameter (gamma) needs to be optimized.\n",
    "\n",
    "Considering our particular dataset, we will select the `linear` kernel.\n",
    "\n",
    "\n",
    "## Penalty Strength\n",
    "\n",
    "The `C` hyperparameter is a penalty term that determines how closely the model will fit to the training set, through a trade-off between smooth decision boundary or a decision boundary attempting to capture the trend in data (gravitating towards underfitting) and classifying the training points correctly (gravitating towards overfitting). \n",
    "\n",
    "For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points. For very tiny values of C, we can get misclassified examples, often even if our training data is linearly separable.\n",
    "\n",
    "This acts in the same way as in a logistic regression as we covered in Part 4(a) of this project.\n",
    "\n",
    "For our model, we will attempt to cycle through a range of `C` values (e.g., `C = 0.01, 0.1, 1, 10, 100` etc.).\n",
    "\n",
    "\n",
    "## Gamma (γ)\n",
    "\n",
    "The `gamma` parameter determines how closely the model will try to fit the training data set. The higher the `gamma` value, the more the model will try to exactly fit the training data.\n",
    "\n",
    "Considering we selected the `linear` kernel, which does not need to optimize for `gamma` parameter, we will have to disregard this parameter (unless we choose a non-linear kernel later on)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9427cd78",
   "metadata": {
    "id": "9427cd78"
   },
   "source": [
    "# Employing Support Vector Classifier\n",
    "\n",
    "We start by instantiating a SVC object for our modelling. Then, following suit, we intend to set up the function to iterate through the two hyperparameters chosen for this model in line with our [hyperparameter selection section above](#Selection-of-Hyperparameters-for-Support-Vector-Classifier).\n",
    "\n",
    "Before attempting hyperparameter optimization though, we will perform a test run with set parameters to see how long the classifier takes to fit to the train data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I2tARNvnUvF9",
   "metadata": {
    "id": "I2tARNvnUvF9"
   },
   "source": [
    "## Performing a Test Run with Set Parameters\n",
    "\n",
    "For the test run, we will choose linear kernel and a `C` value. Unfortunately, the theory for determining a \"good\" starting `C` value is not very well developed at the moment, so we will start with the common `C=1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfef907d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T02:25:32.041331Z",
     "start_time": "2021-10-05T02:25:32.030359Z"
    },
    "id": "dfef907d"
   },
   "outputs": [],
   "source": [
    "# For sake of maintaing the usual pathway, the test run of model fitting is being done with GridSearchCV, for sake of consistency\n",
    "# But this should not matter as we are setting the parameter values to only one value for each\n",
    "\n",
    "# Importing GridSearchCV from scikit-learn library's model_selection module\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wsnjht9WMRRV",
   "metadata": {
    "id": "Wsnjht9WMRRV"
   },
   "outputs": [],
   "source": [
    "# Importing Support Vector Classifier (SVC) from scikit-learn library's SVM module\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Instantiating a SVC object for our modelling (not passing any parameters yet as we will set them and iterate through later)\n",
    "svc_test = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9815edeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T02:42:51.230887Z",
     "start_time": "2021-10-05T02:42:51.213932Z"
    },
    "id": "9815edeb"
   },
   "outputs": [],
   "source": [
    "# Creating a dictionary for the hyperparameters to iterate through and optimize\n",
    "## Performing a test run with set hyperparameter values\n",
    "## Keeping the dictionary structure so it can be used later with more hyperparameter values to iterate through\n",
    "parameters_test = {\n",
    "    'kernel': ['linear'],\n",
    "    'C': [1.0]\n",
    "}\n",
    "\n",
    "# Creating a SVC object for GridSearchCV\n",
    "## Asking the GridSearchCV to iterate through the parameters values we set above\n",
    "## Setting the cross validation fold to 3 considering how computationally expensive SVM already is\n",
    "svc_gscv_test = GridSearchCV(svc, parameters_test, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NKr2cCKbERh8",
   "metadata": {
    "id": "NKr2cCKbERh8"
   },
   "outputs": [],
   "source": [
    "# Fitting to train dataset\n",
    "svc_gscv_test.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaed8a5",
   "metadata": {},
   "source": [
    "It took a staggering number of hours to just fit to a pre-set value of parameters (about 8 hours at the local machine with 16 gigabytes of RAM, and about 4.5 hours at a high-powered machine on Google Colab Pro+ with 52 gigabytes of RAM). We can already consider this model as out of consideration down the road as, in reality when the best model is to be deployed to production, this model will perform extremely slow to generate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3IKNf-w6Ld9D",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3IKNf-w6Ld9D",
    "outputId": "988559d1-6d41-4a7b-9d3d-f1d136103a22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_gscv_test.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f3dde2",
   "metadata": {},
   "source": [
    "Taking a quick look at the GridSearch CV results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h1G4471vLd4r",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h1G4471vLd4r",
    "outputId": "89bd6988-e5be-4eae-fe53-042d26c4003b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5002896032435563"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_gscv_test.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q8WCkK6zLd23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "id": "Q8WCkK6zLd23",
    "outputId": "196d98fe-adcf-433c-d8a9-7ad236e517e3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_kernel</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2034.838971</td>\n",
       "      <td>1.048127</td>\n",
       "      <td>889.446101</td>\n",
       "      <td>1.163496</td>\n",
       "      <td>1</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 1.0, 'kernel': 'linear'}</td>\n",
       "      <td>0.495765</td>\n",
       "      <td>0.503095</td>\n",
       "      <td>0.502009</td>\n",
       "      <td>0.50029</td>\n",
       "      <td>0.00323</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0    2034.838971      1.048127       889.446101        1.163496       1   \n",
       "\n",
       "  param_kernel                          params  split0_test_score  \\\n",
       "0       linear  {'C': 1.0, 'kernel': 'linear'}           0.495765   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.503095           0.502009          0.50029         0.00323   \n",
       "\n",
       "   rank_test_score  \n",
       "0                1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(svc_gscv_test.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64132307",
   "metadata": {
    "id": "QDQs11k8MOTi"
   },
   "source": [
    "So the `mean_test_score` above also does not give us much confidence where with an usual `C` value we are getting a `mean_test_score` (from the validation sets constructed out of the training set) of only 50.03%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3613ae",
   "metadata": {
    "id": "909b9204"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Considering the extremely long runtime even on a high-powered machine, we can consider a SVC out of the race. This will not be practicable to deploy in production given the very-lengthy runtime, and will not be anywhere near a good candidate for solving our business problem, considering the very high performance from the other ML models in the previous parts of this project."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "history_visible": true,
   "machine_shape": "hm",
   "name": "Kazi Shahid_Capstone Part 4d_Sentiment Analysis ML Model 4_Support Vector Machines.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "362px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
